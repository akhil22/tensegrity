\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}

\title{Learning to Walk}
\author{Akhil Nagariya, Vishala Arya, Sameer}
\date{October 2019}

\pdfinfo{%
  /Title    (Learning to walk)
  /Author   (Akhil Nagariya, Vishala Arya, Sameer)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}
% 1.  What is the problem that you will be investigating? Why is it interesting?
% 2.  What is the existing work? What are the reference papers you are using?
% 3. If there are existing implementations/results, will you use them and how?
%     How do you plan to improve or modify such implementations/results?
% 4. Where do you get the data? What simulator or real world
%     domain are you using?
% 5. If you are addressing a theoretical question, how do you plan to make progress?
% 6. How will you evaluate your results? Qualitatively, what kind of results do you
%    expect (e.g. plots, figures, videos, animations)?
%    Quantitatively, what kind of analysis will you use to evaluate and/or compare
%    your results (e.g. what benchmarks, performance metrics, statistical tests)?v 
\begin{document}
\maketitle
  \section{Introduction}
  \section{Related Work}
  \cite{15-17} model the statics of tensegrity robots, \cite{18} used this model to come up with 
  a kinematic controller \cite{22} uses Monte Carlo simulations to learn goal directed behaviors
  requiring thousands of trials making this approach ineffective for real systems. 
  \cite{23} transfers policies learned in simulation to real systems but requires hand tuning. 
  \cite{20} exploited symmetry of a simulated super ball to learn an efficient rolling controller.
  Classical approaches\cite{} for bipedal locomotion uses analytical model for dynamics and hand engineered
  control policies and difficult to generalize for tensegrity robots which are highly underactuated with coupled dynamics.  
  Direct policy search, searches in the space of policies and can effectively scale to 
  complex high dimensional systems but require large number of samples and often get stuck in poor local minima. \cite{pgradient}
  Guided policy search\cite{gps} uses differential dynamic programming to generate suitable guiding samples and uses 
  importance sampling to incorporate these samples directly into the policy. 
  Learning preodic gaits are a key problem in locomotion tasks, 
  \cite{drltens} solves this problem by extending MDGPS\cite{mgpds} to learn a controller for the SUPERball tensegrity structure by sequentially training 
   policies for a perodic behaviour starting from different initial states and then combining them to perform continous locomotion for wide range of
   condions. \cite{drltens} introduces noise in simulation to train policies that are successful in a wide range of simulated terrains. 
  
  \section{Proposed Approach}
  \begin{itemize}
    \item planning to use \cite{DBLP:journals/corr/MontadgomeryL16}
  \end{itemize}
  \cite{gps}
  \cite{vps}
  \cite{2014-cgps}
  \cite{DBLP:journals/corr/LevineFDA15}
  \cite{DBLP:journals/corr/MontgomeryL16}
  \cite{DBLP:journals/corr/GengZBCVSAL16}
  \bibliographystyle{IEEEtran} 
  \bibliography{bibfile}
\end{document}
